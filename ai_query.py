from typing import Dict, Any
import pandas as pd
import mysql.connector
import sqlite3
import re
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

class AIDataQuery:
    """
    AI-powered Data Query Agent
    Combines SQLCoder-7B with 7-layer text processing for Arabic queries
    """

    def __init__(self, db_config: Dict[str, Any] = None, db_type: str = "mysql",
                 db_path: str = None, auto_load_model: bool = True):
        """Initialize the AI DataQuery Agent"""
        print("\n" + "="*60)
        print("ğŸ¤– AI DataQuery Agent - AUTO INITIALIZATION")
        print("="*60)

        self.db_config = db_config
        self.db_type = db_type
        self.conn = None
        self.schema = ""
        self.db_entities = {}

        # Model components
        self.sqlcoder_model = None
        self.sqlcoder_tokenizer = None
        self.sqlcoder_pipeline = None

        # Initialize database
        print("\nğŸ“Š Step 1/2: Initializing Database...")
        db_success = self.initialize_database(db_path)

        if db_success:
            print("âœ… Database connected and schema loaded!")

            if auto_load_model:
                print("\nğŸ¤– Step 2/2: Loading AI Model...")
                print("â³ This may take 2-5 minutes (one time only)...")

                model_success = self.load_sqlcoder_model()

                if model_success:
                    print("âœ… SQLCoder Model loaded successfully!")
                else:
                    print("âš ï¸ Model loading failed - Agent will work in limited mode")
            else:
                print("\nâ­ï¸ Step 2/2: Model loading skipped")

            print("\n" + "="*60)
            print("âœ… Agent is FULLY OPERATIONAL!")
            print("="*60 + "\n")
        else:
            print("âŒ Database initialization failed!")
            print("âš ï¸ Agent cannot function without database connection")
            print("="*60 + "\n")
    # =========================================================================
    # DATABASE METHODS
    # =========================================================================
          
    def get_mysql_connection(self):
        """Create MySQL connection"""
        try:
            conn = mysql.connector.connect(**self.db_config)
            print("âœ… Connected to MySQL database")
            return conn
        except Exception as e:
            print(f"âŒ Database error: {e}")
            return None

    def get_sqlite_connection(self, db_path: str):
        """Create SQLite connection"""
        try:
            conn = sqlite3.connect(db_path)
            print("âœ… Connected to SQLite database")
            return conn
        except Exception as e:
            print(f"âŒ Database error: {e}")
            return None

    def initialize_database(self, db_path: str = None):
        """Initialize database and load schema"""
        try:
            if self.db_type == "mysql":
                self.conn = self.get_mysql_connection()
                if not self.conn:
                    return False
                
                # Get all tables
                query_tables = "SHOW TABLES;"
                tables = pd.read_sql(query_tables, self.conn)
                self.schema = ""

                for table in tables.iloc[:, 0].tolist():
                    cols = pd.read_sql(f"DESCRIBE {table};", self.conn)
                    self.schema += f"\nCREATE TABLE {table} (\n"
                    col_defs = [f"    {row['Field']} {row['Type']}" 
                              for _, row in cols.iterrows()]
                    self.schema += ",\n".join(col_defs)
                    self.schema += "\n);\n"

            else:  # SQLite
                if not db_path:
                    print("âŒ SQLite requires db_path")
                    return False

                self.conn = self.get_sqlite_connection(db_path)
                if not self.conn:
                    return False

                tables = pd.read_sql(
                    "SELECT name FROM sqlite_master WHERE type='table';", 
                    self.conn
                )
                self.schema = ""

                for table in tables['name']:
                    if table != 'sqlite_sequence':
                        cols = pd.read_sql(f"PRAGMA table_info([{table}]);", self.conn)
                        self.schema += f"\nCREATE TABLE {table} (\n"
                        col_defs = [f"    {col['name']} {col['type']}" 
                                  for _, col in cols.iterrows()]
                        self.schema += ",\n".join(col_defs)
                        self.schema += "\n);\n"

            print("âœ… Schema loaded successfully")
            return True

        except Exception as e:
            print(f"âŒ Database error: {e}")
            return False

    # =========================================================================
    # MODEL METHODS
    # =========================================================================

    def load_sqlcoder_model(self):
        """Load SQLCoder-7B model"""
        try:
            import torch
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            print(f"Using device: {device}")

            model_name = "defog/sqlcoder-7b-2"
            print(f"Loading SQLCoder from: {model_name}")

            # Load Tokenizer
            self.sqlcoder_tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True
            )

            if self.sqlcoder_tokenizer.pad_token is None:
                self.sqlcoder_tokenizer.pad_token = self.sqlcoder_tokenizer.eos_token
                self.sqlcoder_tokenizer.pad_token_id = self.sqlcoder_tokenizer.eos_token_id

            # Load Model
            self.sqlcoder_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True,
                load_in_8bit=True
            )
            print("SQLCoder Model loaded successfully")

            # Create Pipeline
            self.sqlcoder_pipeline = pipeline(
                "text-generation",
                model=self.sqlcoder_model,
                tokenizer=self.sqlcoder_tokenizer,
                max_new_tokens=300,
                temperature=0.1,
                do_sample=False,
                return_full_text=False,
                pad_token_id=self.sqlcoder_tokenizer.pad_token_id,
                eos_token_id=self.sqlcoder_tokenizer.eos_token_id
            )

            return True

        except Exception as e:
            print(f"âŒ Model error: {e}")
            return False

    # =========================================================================
    # 7-LAYER TEXT PROCESSING
    # =========================================================================

    def layer1_text_normalization(self, question: str) -> Dict[str, Any]:
        """Layer 1: Arabic text normalization"""
        def normalize_arabic(text: str) -> str:
            if pd.isna(text):
                return ""
            text = str(text).strip()
            text = text.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§")
            text = text.replace("Ø¤", "Ùˆ").replace("Ø¦", "ÙŠ").replace("Ø©", "Ù‡")
            text = text.replace("Ù€", "")
            return text

        normalized = normalize_arabic(question)
        tokens = [word.strip() for word in normalized.split() if word.strip()]

        return {
            'original': question,
            'normalized': normalized,
            'tokens': tokens,
            'clean_text': ' '.join(tokens)
        }

    def layer2_intent_classification(self, processed_text: Dict[str, Any]) -> Dict[str, Any]:
        """Layer 2: Intent classification"""
        text = processed_text['normalized'].lower()

        intent_patterns = {
            'COUNT_QUERY': ['ÙƒÙ…', 'Ø¹Ø¯Ø¯', 'ÙƒØ§Ù…'],
            'LIST_QUERY': ['Ø§Ø³Ù…Ø§Ø¡','Ù…Ø§', 'Ù…Ø§ Ù‡ÙŠ', 'Ø§Ø¹Ø±Ø¶', 'Ø§Ø¸Ù‡Ø±'],
            'AGGREGATE_QUERY': ['Ù…ØªÙˆØ³Ø·', 'Ø§Ø¹Ù„Ù‰','Ø§ÙƒØ«Ø±', 'Ø§Ù‚Ù„', 'Ù…Ø¬Ù…ÙˆØ¹'],
            'FILTER_QUERY': ['Ù…Ù†','Ùˆ', 'Ø§Ù„ØªÙŠ', 'Ø§Ù„Ø°ÙŠÙ†'],
            'JOIN_QUERY': ['Ù…Ø¹Ù‡Ø¯', 'Ù…ÙˆÙ‚Ø¹', 'Ù…Ù†Ø·Ù‚Ø©', 'Ù…Ø±ÙƒØ²', 'Ù…Ù‡Ù†Ø©', 'Ø§Ù‚Ù„ÙŠÙ…']
        }

        detected_intent = 'GENERAL_QUERY'
        confidence = 0.0

        for intent, keywords in intent_patterns.items():
            matches = sum(1 for keyword in keywords if keyword in text)
            if matches > 0:
                current_confidence = matches / len(keywords)
                if current_confidence > confidence:
                    detected_intent = intent
                    confidence = current_confidence

        return {
            'intent': detected_intent,
            'confidence': confidence
        }

    def layer3_entity_recognition(self, processed_text: Dict, db_entities: dict = None) -> dict:
        """Layer 3: Entity extraction"""
        if isinstance(processed_text, str):
            processed_text = {"normalized": processed_text}

        text = processed_text['normalized']
        entities = []

        entity_patterns = {
            "phoneNumber": r"\b07\d{8}\b",
            "nationalID": r"\b\d{10}\b",         # Ø§Ù„Ø±Ù‚Ù… Ø§Ù„ÙˆØ·Ù†ÙŠ - 10 Ø£Ø±Ù‚Ø§Ù…
            "dateOfBirth": r"\b\d{4}-\d{2}-\d{2}\b",  # ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…ÙŠÙ„Ø§Ø¯ (yyyy-mm-dd)
            "mark": r"\b\d{1,3}\b",         # Ø¹Ù„Ø§Ù…Ø© Ø§Ù…ØªØ­Ø§Ù† (0-100 Ø£Ùˆ Ø£ÙƒØ«Ø±)
            'regionName': {
                'Ø§Ù„Ø´Ù…Ø§Ù„ÙŠ': 'NORTHERN',
                'Ø´Ù…Ø§Ù„ÙŠ': 'NORTHERN',
                'Ø§Ù„Ø¬Ù†ÙˆØ¨ÙŠ': 'SOUTHERN',
                'Ø¬Ù†ÙˆØ¨ÙŠ': 'SOUTHERN',
                'Ø§Ù„Ø£ÙˆØ³Ø·': 'CENTRAL',
                'Ø§ÙˆØ³Ø·': 'CENTRAL',
                'Ø£ÙˆØ³Ø·': 'CENTRAL',
                'Ø§Ù„Ø´Ù…Ø§Ù„': 'NORTHERN',
                'Ø§Ù„Ø¬Ù†ÙˆØ¨': 'SOUTHERN',
                'Ø§Ù„ÙˆØ³Ø·': 'CENTRAL'
            },
            'status': {
                'Ù…ØªÙ‚Ø¯Ù…': 'PENDING',
                'Ù…Ù„ØªØ­Ù‚': 'PENDING',
                'Ù‚ÙŠØ¯': 'PHONE_CALL',
                'Ø§ØªØµØ§Ù„': 'PHONE_CALL',
                'Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª': 'WAITING_FOR_DOCUMENTS',
                'Ù‚Ø§Ø¦Ù…Ø© Ø§Ù†ØªØ¸Ø§Ø±': 'WAITING_FOR_DOCUMENTS',
                'Ù†Ø§Ø¬Ø­': 'PASSED_THE_EXAM',
                'Ù…Ù‚Ø¨ÙˆÙ„': 'ACCEPTED',
                'Ù‚Ø¨ÙˆÙ„': 'ACCEPTED',
                'Ù…Ø±ÙÙˆØ¶': 'REJECTED',
                'Ø±ÙØ¶': 'REJECTED'
            },
            'profession': {
                'Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ': 'Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠ',
                'Ù…Ø´ØºÙ„': 'Ù…Ø´ØºÙ„',
                'Ø­Ù„Ø§Ù‚': 'Ø­Ù„Ø§Ù‚',
                'ØªÙ…Ø¯ÙŠØ¯Ø§Øª': 'ØªÙ…Ø¯ÙŠØ¯Ø§Øª',
                'Ø®ÙŠØ§Ø·': 'Ø®ÙŠØ§Ø·',
                'Ø¯Ù‡Ø§Ù†': 'Ø¯Ù‡Ø§Ù†' ,
                'Ù…Ø¬Ù‡Ø²': 'Ù…Ø¬Ù‡Ø²',
                'Ù…Ø±ÙƒØ¨': 'Ù…Ø±ÙƒØ¨',
                'Ù„Ø­ÙŠÙ…': 'Ù„Ø­ÙŠÙ…',
                'Ø­Ø¯Ø§Ø¯': 'Ø­Ø¯Ø§Ø¯',
                'Ù†Ø¬Ø§Ø±': 'Ù†Ø¬Ø§Ø±',
                'ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠ': 'ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠ',
                'Ø¯Ø§Ø¹Ù…': 'Ø¯Ø§Ø¹Ù…',
                'Ù‚ØµÙŠØ±': 'Ù‚ØµÙŠØ±' ,
                'Ø¨Ù„ÙŠØ·': 'Ø¨Ù„ÙŠØ·',
                'Ù…Ø²Ø§Ø±Ø¹': 'Ù…Ø²Ø§Ø±Ø¹',
                'Ø§Ù„ØªØ³ÙˆÙŠÙ‚': 'Ø§Ù„ØªØ³ÙˆÙŠÙ‚' ,
                'Ø·Ø§Ù‚Ø© Ø´Ù…Ø³ÙŠØ©': 'Ø·Ø§Ù‚Ø© Ø´Ù…Ø³ÙŠØ©'
            },
            'gender': {
                'Ø°ÙƒØ±': 'MALE',
                'Ø°ÙƒÙˆØ±': 'MALE',
                'Ø§Ù†Ø«Ù‰': 'FEMALE',
                'Ø§Ù†Ø§Ø«': 'FEMALE',
                'Ø£Ù†Ø«Ù‰': 'FEMALE'
            },
            'educationLevel': {
                'Ø¨ÙƒØ§Ù„ÙˆØ±ÙŠÙˆØ³': 'BACKELOR',
                'Ø¯Ø¨Ù„ÙˆÙ…': 'DIPLOMA',
                'Ù…Ø§Ø¬Ø³ØªÙŠØ±': 'MASTER',
                'Ù…Ø§Ø³ØªØ±': 'MASTER',
                'Ø«Ø§Ù†ÙˆÙŠØ© Ø¹Ø§Ù…Ø©': 'HIGH_SCHOOL',
                'Ø«Ø§Ù†ÙˆÙŠ': 'HIGH_SCHOOL',
                'Ø«Ø§Ù†ÙˆÙŠØ©': 'HIGH_SCHOOL',
                'ØªÙˆØ¬ÙŠÙ‡ÙŠ': 'HIGH_SCHOOL',
                'Ø§Ø¹Ø¯Ø§Ø¯ÙŠ': 'MIDDLE_SCHOOL',
                'Ø§Ø¹Ø¯Ø§Ø¯ÙŠØ©': 'MIDDLE_SCHOOL',
                'Ù…ØªÙˆØ³Ø·': 'MIDDLE_SCHOOL'

            },
            'area': {
                'Ø§Ù„Ø²Ø±Ù‚Ø§Ø¡': 'Ø§Ù„Ø²Ø±Ù‚Ø§Ø¡',
                'Ø§Ù„Ù…ÙˆÙ‚Ø±': 'Ø§Ù„Ù…ÙˆÙ‚Ø±',
                'Ù…Ø§Ø¯Ø¨Ø§': 'Ù…Ø§Ø¯Ø¨Ø§',
                'Ù…Ø§Ø±ÙƒØ§ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ': 'Ù…Ø§Ø±ÙƒØ§ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ',
                'Ø°ÙŠØ¨Ø§Ù† ': 'Ø°ÙŠØ¨Ø§Ù†',
                'Ø§Ù„ÙƒØ±Ùƒ': 'Ø§Ù„ÙƒØ±Ùƒ',
                'Ù…Ø¹Ø§Ù†': 'Ù…Ø¹Ø§Ù†' ,
                'Ø§Ù„Ø·ÙÙŠÙ„Ø©': 'Ø§Ù„Ø·ÙÙŠÙ„Ø©',
                'Ø§Ù„Ù‚ÙˆÙŠØ±Ø©': 'Ø§Ù„Ù‚ÙˆÙŠØ±Ø©',
                'Ø§Ù„Ø±ÙŠØ´Ø©': 'Ø§Ù„Ø±ÙŠØ´Ø© ',
                'Ø§Ù„Ø³Ø±Ø­Ø§Ù†': 'Ø§Ù„Ø³Ø±Ø­Ø§Ù†',
                'Ø¹Ø¬Ù„ÙˆÙ†': 'Ø¹Ø¬Ù„ÙˆÙ†',
                'Ø§Ù„Ø±Ù…Ø«Ø§': 'Ø§Ù„Ø±Ù…Ø«Ø§',
                'Ø¬Ø±Ø´': 'Ø¬Ø±Ø´',
                'Ø§Ù„Ø¬ÙØ±': 'Ø§Ù„Ø¬ÙØ±' ,
                'Ø§Ù„Ù…ÙØ±Ù‚': 'Ø§Ù„Ù…ÙØ±Ù‚',
                'Ø§Ù„ÙƒÙˆØ±Ø©': 'Ø§Ù„ÙƒÙˆØ±Ø©'
            },
            'institute': {
                'Ù…Ø¹Ù‡Ø¯ Ø§Ù„Ø²Ø±Ù‚Ø§Ø¡': 'Ù…Ø¹Ù‡Ø¯ Ø§Ù„Ø²Ø±Ù‚Ø§Ø¡',
                'Ù…Ø¹Ù‡Ø¯ Ø§Ù„Ù…ÙˆÙ‚Ø±': 'Ù…Ø¹Ù‡Ø¯ Ø§Ù„Ù…ÙˆÙ‚Ø±',
                'Ù…Ø¹Ù‡Ø¯ Ù…Ø§Ø¯Ø¨Ø§': 'Ù…Ø¹Ù‡Ø¯ Ù…Ø§Ø¯Ø¨Ø§',
                'Ù…Ø¹Ù‡Ø¯ Ù…Ø§Ø±ÙƒØ§ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ': 'Ù…Ø¹Ù‡Ø¯ Ù…Ø§Ø±ÙƒØ§ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ',
                'Ù…Ø¹Ù‡Ø¯ Ø°ÙŠØ¨Ø§Ù† ': 'Ù…Ø¹Ù‡Ø¯ Ø°ÙŠØ¨Ø§Ù†',
                'Ù…Ø¹Ù‡Ø¯ Ø§Ù„ÙƒØ±Ùƒ': 'Ù…Ø¹Ù‡Ø¯ Ø§Ù„ÙƒØ±Ùƒ',
                'Ù…Ø¹Ù‡Ø¯ Ù…Ø¹Ø§Ù†': 'Ù…Ø¹Ù‡Ø¯ Ù…Ø¹Ø§Ù†' ,
                'Ù…Ø¹Ù‡Ø¯ Ø§Ù„Ø·ÙÙŠÙ„Ø©': 'Ù…Ø¹Ù‡Ø¯ Ø§Ù„Ø·ÙÙŠÙ„Ø©',
                'Ù…Ø¹Ù‡Ø¯ Ø§Ù„Ù‚ÙˆÙŠØ±Ø©': 'Ù…Ø¹Ù‡Ø¯ Ø§Ù„Ù‚ÙˆÙŠØ±Ø©',
                'Ù…Ø¹Ù‡Ø¯ Ø§Ù„Ø±ÙŠØ´Ø©': 'Ù…Ø¹Ù‡Ø¯ Ø§Ù„Ø±ÙŠØ´Ø© ',
                'Ù…Ø¹Ù‡Ø¯ Ø§Ù„Ø³Ø±Ø­Ø§Ù†': 'Ù…Ø¹Ù‡Ø¯Ø§Ù„Ø³Ø±Ø­Ø§Ù†',
                'Ù…Ø¹Ù‡Ø¯ Ø¹Ø¬Ù„ÙˆÙ†': 'Ù…Ø¹Ù‡Ø¯ Ø¹Ø¬Ù„ÙˆÙ†',
                'Ù…Ø¹Ù‡Ø¯ Ù…Ø§Ø¯Ø¨Ø§': 'Ù…Ø¹Ù‡Ø¯ Ø§Ù„Ø±Ù…Ø«Ø§',
                'Ù…Ø¹Ù‡Ø¯ Ø¬Ø±Ø´': 'Ù…Ø¹Ù‡Ø¯ Ø¬Ø±Ø´',
                'Ù…Ø¹Ù‡Ø¯ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ø´Ù…Ø³ÙŠØ©': 'Ù…Ø±ÙƒØ² Ø§Ù„ØªÙ…ÙŠØ² Ø§Ù„Ø£Ø±Ø¯Ù†ÙŠ Ø§Ù„Ø£Ù„Ù…Ø§Ù†ÙŠ Ù„Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ø´Ù…Ø³ÙŠØ©' ,
                'Ù…Ø¹Ù‡Ø¯ Ø§Ù„Ø¬ÙØ±': 'Ù…Ø¹Ù‡Ø¯ Ø§Ù„Ø¬ÙØ±',
                'Ù…Ø¹Ù‡Ø¯ Ø§Ù„ÙƒÙˆØ±Ø©': 'Ù…Ø¹Ù‡Ø¯ Ø§Ù„ÙƒÙˆØ±Ø©'
            }
              }

        if db_entities:
            entity_patterns.update(db_entities)

        for entity_type, patterns in entity_patterns.items():
            if isinstance(patterns, str):  # regex
                match = re.search(patterns, text)
                if match:
                    entities.append({
                        'type': entity_type,
                        'value': match.group(),
                        'original': match.group()
                    })
            elif isinstance(patterns, dict):  # dictionary
                for pattern, standard_value in patterns.items():
                    if pattern in text:
                        entities.append({
                            'type': entity_type,
                            'value': standard_value,
                            'original': pattern
                        })

        return {
            'entities': entities,
            'entity_count': len(entities),
            'entity_types': list(set([e['type'] for e in entities]))
        }

    def layer4_context_analysis(self, processed_text: Dict[str, Any], 
                              intent: Dict[str, Any], 
                              entities: Dict[str, Any]) -> Dict[str, Any]:
        """Layer 4: Context analysis"""
        text = processed_text['normalized']

        table_indicators = {
            'userForm': ['Ø·Ø§Ù„Ø¨', 'Ø·Ù„Ø§Ø¨','Ù…Ø³Ø¬Ù„','Ù…ØªØ¯Ø±Ø¨', 'Ø¯Ø§Ø±Ø³'],
            'regions': ['Ø§Ù‚Ù„ÙŠÙ…', 'Ø§Ù„Ø§Ù‚Ù„ÙŠÙ…', 'Ø§Ù‚Ø§Ù„ÙŠÙ…'],
            'areas': ['Ù…Ù†Ø·Ù‚Ø©', 'Ù…ÙƒØ§Ù† Ø§Ù„Ø³ÙƒÙ†', 'Ø³ÙƒØ§Ù†', 'Ù…ÙˆÙ‚Ø¹'],
            'institutes': ['Ù…Ø¹Ù‡Ø¯', 'Ù…Ø¹Ø§Ù‡Ø¯', 'Ù…Ø±Ø§ÙƒØ²', 'Ù…Ø±ÙƒØ²', 'Ù…Ø¤Ø³Ø³Ø©'],
            'professions': ['Ù…Ù‡Ù†Ø©', 'Ù…Ù‡Ù†ØªÙ‡', 'Ø§Ù„Ù…Ù‡Ù†Ø©']
        }

        required_tables = []
        for table, indicators in table_indicators.items():
            if any(indicator in text for indicator in indicators):
                required_tables.append(table)

        if not required_tables:
            required_tables = ['userForm']

        join_needed = len(required_tables) > 1

        return {
            'required_tables': required_tables,
            'join_needed': join_needed,
            'complexity_level': 'HIGH' if join_needed else 'LOW'
        }

    def layer5_schema_mapping(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Layer 5: Schema mapping"""
        schema_info = {}
        current_table = None

        for line in self.schema.split('\n'):
            line = line.strip()
            if line.startswith('CREATE TABLE'):
                current_table = line.split()[2].strip('(')
                schema_info[current_table] = []
            elif current_table and line and not line.startswith('CREATE'):
                schema_info[current_table].append(line)

        relevant_schema = {}
        for table in context['required_tables']:
            if table in schema_info:
                relevant_schema[table] = schema_info[table]

        return {
            'relevant_schema': relevant_schema,
            'full_schema': schema_info
        }

    def layer6_prompt_optimization(self, question: str, intent: Dict, 
                                 entities: Dict, context: Dict, 
                                 schema_mapping: Dict) -> str:
        """Layer 6: Prompt optimization"""
        prompt = f"""
### Task
Generate a SQL query to answer the following Arabic question: `{question}`

### Database Schema
{self.schema}

### Additional Context
- Question Type: {intent['intent']}
- Entities: {', '.join([e['value'] for e in entities['entities']])}
- Tables Needed: {', '.join(context['required_tables'])}
- For Arabic text matching, use LIKE '%text%'
- IMPORTANT: Do NOT use ILIKE. Use only LIKE. SQLite does not support ILIKE.
  Example: use "WHERE column LIKE '%text%'" instead of "ILIKE".

### Relationships:
    - userForm.region â†’ regions.name
    - userForm.area â†’ areas.name
    - userForm.institute â†’ institutes.name
    - areas.regionName â†’ regions.name
    - institutes.areaName â†’ areas.name
    - institutes.regionName â†’ regions.name
    - professions.areaName â†’ areas.name
    - professions.regionName â†’ regions.name


### Answer
Given the database schema, here is the SQL query that answers `{question}`:
```sql
"""
        return prompt

    def layer7_response_processing(self, raw_response: str) -> str:
        """Layer 7: Process SQLCoder response"""
        if not raw_response:
            return None
        # Extract from code block
        sql_block_match = re.search(r'```sql\n(.*?)```', raw_response, re.DOTALL)
        if sql_block_match:
            sql = sql_block_match.group(1).strip()
            return sql.replace(';', '').strip()
        # Extract SELECT statement
        lines = raw_response.split('\n')
        for line in lines:
            line = line.strip()
            if line.upper().startswith('SELECT'):
                return line.replace(';', '').strip()
        # Regex fallback
        sql_match = re.search(r'SELECT[^;]*', raw_response, re.IGNORECASE | re.DOTALL)
        if sql_match:
            return sql_match.group(0).strip().replace(';', '')

        return None


    # =========================================================================
    # MAIN PROCESSING METHODS
    # =========================================================================

    def send_to_sqlcoder(self, prompt: str) -> str:
        """Send prompt to SQLCoder"""
        if not self.sqlcoder_pipeline:
            print("SQLCoder not available!")
            return None

        try:
            response = self.sqlcoder_pipeline(prompt, max_new_tokens=300)
            if isinstance(response, list) and len(response) > 0:
                return response[0].get('generated_text', '')
            return str(response)
        except Exception as e:
            print(f"Error calling SQLCoder: {e}")
            return None

    def process_sql_question(self, question: str) -> Dict[str, Any]:
        """Process question through all layers"""
        print(f"Processing question: {question}")
        print("=" * 60)

        layer1_result = self.layer1_text_normalization(question)
        print(f"Layer 1: Normalized text")

        layer2_result = self.layer2_intent_classification(layer1_result)
        print(f"Layer 2: Intent: {layer2_result['intent']}")

        layer3_result = self.layer3_entity_recognition(layer1_result)
        print(f"Layer 3: Found {layer3_result['entity_count']} entities")

        layer4_result = self.layer4_context_analysis(
            layer1_result, layer2_result, layer3_result
        )
        print(f"Layer 4: Required tables: {layer4_result['required_tables']}")

        layer5_result = self.layer5_schema_mapping(layer4_result)
        print(f"Layer 5: Schema mapped: {len(layer5_result['relevant_schema'])} tables")

        optimized_prompt = self.layer6_prompt_optimization(
            question, layer2_result, layer3_result, 
            layer4_result, layer5_result
        )
        print(f"Layer 6: Prompt created ({len(optimized_prompt)} chars)")

        # Send to SQLCoder
        print("Sending to SQLCoder...")
        raw_response = self.send_to_sqlcoder(optimized_prompt)
        
        # Layer 7
        final_sql = self.layer7_response_processing(raw_response)
        print(f"Layer 7: SQL generated")

        return {
            'question': question,
            'final_sql': final_sql,
            'success': final_sql is not None,
            'raw_response': raw_response
        }

    def classify_intent(self, user_text: str) -> str:
        """Classify user intent"""
        lower_text = user_text.lower()

        chat_keywords = ['Ù…Ø±Ø­Ø¨Ø§', 'Ø§Ù‡Ù„Ø§', 'Ø§Ù„Ø³Ù„Ø§Ù…', 'ØµØ¨Ø§Ø­', 'Ù…Ø³Ø§Ø¡', 
                        'ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ', 'Ø´Ùˆ Ø§Ø®Ø¨Ø§Ø±Ùƒ', 'Ù‡Ø§ÙŠ', 'Ù‡Ù„Ø§', 'Ø´ÙƒØ±Ø§', 
                        'Ø¨Ø§ÙŠ', 'ÙˆØ¯Ø§Ø¹Ø§', 'Ù…Ù† Ø§Ù†Øª', 'Ø´Ùˆ Ø§Ù†Øª', 'ÙƒÙŠÙÙƒ']

        if any(kw in lower_text for kw in chat_keywords):
            return "chat"

        sql_keywords = ['ÙƒÙ…', 'Ø¹Ø¯Ø¯', 'Ø§Ø³Ù…Ø§Ø¡', 'Ø§Ø¹Ø±Ø¶', 'Ø§Ø¸Ù‡Ø±', 'Ù…Ù†', 
                       'Ù…Ø§', 'Ø·Ø§Ù„Ø¨', 'Ø·Ù„Ø§Ø¨', 'Ø¹Ù„Ø§Ù…Ø©', 'Ù…Ø¹Ù‡Ø¯', 'Ù…Ù†Ø·Ù‚Ø©']

        if any(kw in lower_text for kw in sql_keywords):
            return "sql"

        return "general"

    def chat_response(self, user_text: str) -> str:
        """Generate chat response"""
        lower_text = user_text.lower()

        if any(w in lower_text for w in ['Ù…Ø±Ø­Ø¨Ø§', 'Ø§Ù‡Ù„Ø§', 'Ø§Ù„Ø³Ù„Ø§Ù…']):
            return """Ù…Ø±Ø­Ø¨Ø§Ù‹! ğŸ‘‹ 
Ø£Ù†Ø§ Ø¨ÙˆØª Ø°ÙƒÙŠ Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¹Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.

ğŸ¯ Ø¬Ø±Ø¨ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©:
â€¢ ÙƒÙ… Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ù„Ø§Ø¨ØŸ
â€¢ Ù…Ù† Ø§Ù„Ø·Ù„Ø§Ø¨ ÙÙŠ Ø§Ù„Ø´Ù…Ø§Ù„ØŸ
â€¢ ÙƒÙ… Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ù†Ø§Ø«ØŸ"""

        if any(w in lower_text for w in ['ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ', 'Ø´Ùˆ Ø§Ø®Ø¨Ø§Ø±Ùƒ']):
            return "Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡! ğŸ˜Š ÙƒÙŠÙ Ø£Ù‚Ø¯Ø± Ø£Ø³Ø§Ø¹Ø¯ÙƒØŸ"

        if any(w in lower_text for w in ['Ù…Ù† Ø§Ù†Øª', 'Ø´Ùˆ Ø§Ù†Øª']):
            return """Ø£Ù†Ø§ Ø¨ÙˆØª Ø°ÙƒÙŠ ğŸ¤– 
Ø£Ø³Ø§Ø¹Ø¯ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¹Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠ!"""

        if any(w in lower_text for w in ['Ø´ÙƒØ±Ø§', 'Ù…Ø´ÙƒÙˆØ±']):
            return "Ø§Ù„Ø¹ÙÙˆ! ğŸŒŸ"

        return """Ù…Ø±Ø­Ø¨Ø§Ù‹! ğŸ‘‹ ÙƒÙŠÙ Ø£Ù‚Ø¯Ø± Ø£Ø³Ø§Ø¹Ø¯ÙƒØŸ

ğŸ’¡ Ø¬Ø±Ø¨:
â€¢ ÙƒÙ… Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ù„Ø§Ø¨ØŸ
â€¢ Ù…Ù† Ø§Ù„Ø·Ù„Ø§Ø¨ ÙÙŠ Ø§Ù„Ø´Ù…Ø§Ù„ØŸ
â€¢ ÙƒÙ… Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ù†Ø§Ø«ØŸ"""

    def execute_query(self, question: str) -> Dict[str, Any]:
        """Execute query with enhanced processing"""
        intent = self.classify_intent(question)
        print(f"\nğŸ¯ Intent: {intent}")

        if intent == "chat":
            return {
                'success': True,
                'intent': 'chat',
                'answer': self.chat_response(question),
                'sql': None,
                'results': None
            }

        if intent == "general":
            return {
                'success': False,
                'intent': 'general',
                'answer': "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ÙÙ‡Ù… Ø§Ù„Ø³Ø¤Ø§Ù„. Ø¬Ø±Ø¨ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¢Ø®Ø±!",
                'sql': None,
                'results': None
            }

        result = self.process_sql_question(question)
        if not result['success']:
            return {
                'success': False,
                'intent': 'sql',
                'answer': 'âš ï¸ Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ÙÙ‡Ù… Ø§Ù„Ø³Ø¤Ø§Ù„',
                'sql': None,
                'results': None,
                'debug': result.get('raw_response')
            }
              # Execute SQL
        max_attempts = 2
        last_error = None
        for attempt in range(max_attempts):
            try:
                df = pd.read_sql(result['final_sql'], self.conn)
                results_list = df.to_dict('records')

                if len(results_list) == 1 and len(results_list[0]) == 1:
                    value = list(results_list[0].values())[0]
                    answer = f"âœ… Ø§Ù„Ù†ØªÙŠØ¬Ø©: {value}"
                elif len(results_list) > 0:
                    answer = f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(results_list)} Ù†ØªÙŠØ¬Ø©"
                else:
                    answer = "â„¹ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬"

                return {
                    'success': True,
                    'intent': 'sql',
                    'answer': answer,
                    'sql': result['final_sql'],
                    'results': results_list[:]
                }
            except Exception as e:
                last_error = str(e)
                print(f"âŒ Attempt {attempt + 1} failed: {last_error}")

                if attempt < max_attempts - 1:
                    sql_query = result['final_sql']
                    if "ILIKE" in sql_query:
                        sql_query = sql_query.replace("ILIKE", "LIKE COLLATE NOCASE")
                    if "ilike" in sql_query:
                        sql_query = sql_query.replace("ilike", "LIKE COLLATE NOCASE")

                    if not result['final_sql'].endswith(';'):
                        result['final_sql'] += ';'
        return {
            'success': False,
            'intent': 'sql',
            'answer': "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ÙÙ‡Ù… Ø§Ù„Ø³Ø¤Ø§Ù„. Ø¬Ø±Ø¨ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¢Ø®Ø±!",
            'sql': None,
            'results': None
        }
                    

    def query(self, question: str) -> Dict[str, Any]:
        """Main query method"""
        return self.execute_query(question)

    def close(self):
        """Close database connection"""
        if self.conn:
            self.conn.close()
            print("âœ… Database connection closed")